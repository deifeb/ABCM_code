"""
ç»¼åˆæ¨¡å‹è¯„ä¼°é›†æˆç¤ºä¾‹ - Model Evaluation Integration Example

è¯¥ç¤ºä¾‹å±•ç¤ºå¦‚ä½•å°†ä»ABCM_RAF.pyæå–çš„ç»¼åˆæ¨¡å‹è¯„ä¼°æ¨¡å—é›†æˆåˆ°ABCMç³»ç»Ÿä¸­ï¼Œ
ç”¨äºå»ºç«‹ç‰¹å¾-æ¨¡å‹åŒ¹é…å…³ç³»ï¼Œä¸ºAgent 3çš„å…ƒå­¦ä¹ å™¨æä¾›è®­ç»ƒæ•°æ®ã€‚

This example demonstrates how to integrate the comprehensive model evaluation module 
extracted from ABCM_RAF.py into the ABCM system to establish feature-model matching 
relationships and provide training data for Agent 3's meta-learner.

ä½œè€… Author: ABCM Team
åˆ›å»ºæ—¶é—´ Created: 2024
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, List
import warnings

# å¯¼å…¥ABCMç»„ä»¶ Import ABCM components
from agents import FeatureExtractionAgent, ClassificationAgent
from comprehensive_model_evaluation import ComprehensiveModelEvaluator

warnings.filterwarnings('ignore')


class ABCMModelEvaluationPipeline:
    """
    ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿
    ABCM Model Evaluation Pipeline
    
    é›†æˆç‰¹å¾æå–ã€åˆ†ç±»èšç±»å’Œç»¼åˆæ¨¡å‹è¯„ä¼°ï¼Œä¸ºå…ƒå­¦ä¹ å™¨ç”Ÿæˆè®­ç»ƒæ•°æ®
    Integrates feature extraction, classification clustering, and comprehensive model evaluation 
    to generate training data for meta-learner
    """
    
    def __init__(self, 
                 encoding_dim: int = 12,
                 evaluation_epochs: int = 10,
                 prediction_length: int = 6):
        """
        åˆå§‹åŒ–è¯„ä¼°æµæ°´çº¿
        Initialize evaluation pipeline
        
        å‚æ•° Args:
            encoding_dim (int): è‡ªç¼–ç å™¨ç»´åº¦ Autoencoder dimension
            evaluation_epochs (int): è¯„ä¼°è½®æ•° Number of evaluation epochs
            prediction_length (int): é¢„æµ‹é•¿åº¦ Prediction length
        """
        # åˆå§‹åŒ–Agent 1: ç‰¹å¾æå–ä»£ç† Initialize Agent 1: Feature Extraction Agent
        self.agent1 = FeatureExtractionAgent(
            encoding_dim=encoding_dim,
            autoencoder_epochs=50,
            batch_size=16
        )
        
        # åˆå§‹åŒ–Agent 2: åˆ†ç±»ä»£ç† Initialize Agent 2: Classification Agent
        self.agent2 = ClassificationAgent(
            accuracy_threshold=0.9,
            max_clusters=10
        )
        
        # åˆå§‹åŒ–ç»¼åˆæ¨¡å‹è¯„ä¼°å™¨ Initialize comprehensive model evaluator
        self.model_evaluator = ComprehensiveModelEvaluator(
            evaluation_epochs=evaluation_epochs,
            prediction_length=prediction_length,
            freq='M'
        )
        
        print("ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
        print("ABCM Model Evaluation Pipeline initialized")
        
    def extract_features_and_cluster(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, np.array, pd.DataFrame]:
        """
        æ­¥éª¤1-2: ç‰¹å¾æå–å’Œèšç±»åˆ†æ
        Steps 1-2: Feature extraction and clustering analysis
        
        å‚æ•° Args:
            data (pd.DataFrame): åŸå§‹æ—¶é—´åºåˆ—æ•°æ® Original time series data
            
        è¿”å› Returns:
            tuple: (features, cluster_labels, grouped_data) ç‰¹å¾ã€èšç±»æ ‡ç­¾å’Œåˆ†ç»„ä¿¡æ¯
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤1-2: ç‰¹å¾æå–å’Œèšç±»åˆ†æ")
        print("STEPS 1-2: FEATURE EXTRACTION AND CLUSTERING ANALYSIS")
        print("=" * 80)
        
        # æ­¥éª¤1: ä½¿ç”¨Agent 1æå–ç‰¹å¾ Step 1: Extract features using Agent 1
        print("ğŸ”§ æ­£åœ¨æå–ç‰¹å¾... Extracting features...")
        features = self.agent1.extract_features(data, strategy='combined')
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ: {features.shape[1]}ä¸ªç‰¹å¾")
        print(f"âœ… Feature extraction completed: {features.shape[1]} features")
        
        # æ­¥éª¤2: ä½¿ç”¨Agent 2è¿›è¡Œèšç±» Step 2: Perform clustering using Agent 2
        print("ğŸ”§ æ­£åœ¨è¿›è¡Œèšç±»åˆ†æ... Performing clustering analysis...")
        cluster_labels, evaluation = self.agent2.classify(features, use_pca=True, use_cosine_similarity=True)
        
        n_clusters = len(np.unique(cluster_labels))
        print(f"âœ… èšç±»åˆ†æå®Œæˆ: {n_clusters}ä¸ªèšç±»")
        print(f"âœ… Clustering analysis completed: {n_clusters} clusters")
        print(f"èšç±»å‡†ç¡®åº¦ Clustering accuracy: {evaluation['accuracy']:.3f}")
        
        # åˆ›å»ºåˆ†ç»„æ•°æ® Create grouped data
        grouped_data = self._create_grouped_data(data.columns.tolist(), cluster_labels)
        
        return features, cluster_labels, grouped_data
    
    def _create_grouped_data(self, series_names: List[str], cluster_labels: np.array) -> pd.DataFrame:
        """
        åˆ›å»ºåˆ†ç»„æ•°æ®ä¿¡æ¯
        Create grouped data information
        
        å‚æ•° Args:
            series_names (list): æ—¶é—´åºåˆ—åç§°åˆ—è¡¨ List of time series names
            cluster_labels (np.array): èšç±»æ ‡ç­¾ Cluster labels
            
        è¿”å› Returns:
            pd.DataFrame: åˆ†ç»„æ•°æ®ä¿¡æ¯ Grouped data information
        """
        # åˆ›å»ºDataFrame mapping series to clusters
        series_cluster_mapping = pd.DataFrame({
            'data_names': series_names,
            'category': cluster_labels
        })
        
        # æŒ‰ç±»åˆ«åˆ†ç»„ Group by category
        grouped_data = series_cluster_mapping.groupby('category')['data_names'].apply(list).reset_index()
        
        return grouped_data
    
    def comprehensive_model_evaluation(self, 
                                     data: pd.DataFrame,
                                     grouped_data: pd.DataFrame,
                                     save_results: bool = True) -> Tuple[List, pd.DataFrame]:
        """
        æ­¥éª¤3: ç»¼åˆæ¨¡å‹è¯„ä¼°
        Step 3: Comprehensive model evaluation
        
        å‚æ•° Args:
            data (pd.DataFrame): åŸå§‹æ—¶é—´åºåˆ—æ•°æ® Original time series data
            grouped_data (pd.DataFrame): åˆ†ç»„æ•°æ®ä¿¡æ¯ Grouped data information
            save_results (bool): æ˜¯å¦ä¿å­˜ç»“æœ Whether to save results
            
        è¿”å› Returns:
            tuple: (errors_by_epoch, final_error_matrix) è¯„ä¼°ç»“æœ
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤3: ç»¼åˆæ¨¡å‹è¯„ä¼°")
        print("STEP 3: COMPREHENSIVE MODEL EVALUATION")
        print("=" * 80)
        
        # è¿è¡Œç»¼åˆæ¨¡å‹è¯„ä¼° Run comprehensive model evaluation
        errors_by_epoch, final_error_matrix = self.model_evaluator.comprehensive_evaluation(
            grouped_data=grouped_data,
            data=data,
            save_results=save_results,
            results_dir="model_evaluation_results"
        )
        
        return errors_by_epoch, final_error_matrix
    
    def prepare_meta_learning_data(self, 
                                 features: pd.DataFrame,
                                 cluster_labels: np.array,
                                 final_error_matrix: pd.DataFrame) -> Tuple[pd.DataFrame, np.array]:
        """
        æ­¥éª¤4: å‡†å¤‡å…ƒå­¦ä¹ æ•°æ®
        Step 4: Prepare meta-learning data
        
        å‚æ•° Args:
            features (pd.DataFrame): ç‰¹å¾æ•°æ® Feature data
            cluster_labels (np.array): èšç±»æ ‡ç­¾ Cluster labels
            final_error_matrix (pd.DataFrame): æœ€ç»ˆé”™è¯¯çŸ©é˜µ Final error matrix
            
        è¿”å› Returns:
            tuple: (X_meta, y_meta) å…ƒå­¦ä¹ çš„è¾“å…¥å’Œè¾“å‡ºæ•°æ®
        """
        print("\n" + "=" * 80)
        print("æ­¥éª¤4: å‡†å¤‡å…ƒå­¦ä¹ æ•°æ®")
        print("STEP 4: PREPARE META-LEARNING DATA")
        print("=" * 80)
        
        # ç»“åˆç‰¹å¾ä¸èšç±»æ ‡ç­¾ Combine features with cluster labels
        meta_features = features.copy()
        meta_features['cluster_label'] = cluster_labels
        
        # ä¸ºæ¯ä¸ªå¤‡ä»¶ç¡®å®šæœ€ä½³æ¨¡å‹ Determine best model for each spare part
        best_models = []
        for i, spare_part_cluster in enumerate(cluster_labels):
            # æ‰¾åˆ°è¯¥å¤‡ä»¶æ‰€å±èšç±»çš„æœ€ä½³æ¨¡å‹ Find best model for the cluster this spare part belongs to
            cluster_row = final_error_matrix.iloc[spare_part_cluster]
            best_model = cluster_row.idxmin()
            best_models.append(best_model)
        
        # ç¼–ç ç›®æ ‡æ ‡ç­¾ Encode target labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        y_meta = label_encoder.fit_transform(best_models)
        
        print(f"âœ… å…ƒå­¦ä¹ æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"âœ… Meta-learning data preparation completed")
        print(f"ç‰¹å¾ç»´åº¦ Feature dimensions: {meta_features.shape}")
        print(f"ç›®æ ‡æ ‡ç­¾æ•°é‡ Number of target labels: {len(np.unique(y_meta))}")
        
        # æ˜¾ç¤ºç›®æ ‡æ¨¡å‹åˆ†å¸ƒ Display target model distribution
        from collections import Counter
        model_counts = Counter(best_models)
        print("\nğŸ¯ ç›®æ ‡æ¨¡å‹åˆ†å¸ƒ Target model distribution:")
        for model, count in model_counts.items():
            print(f"  {model}: {count}ä¸ªå¤‡ä»¶ spare parts")
        
        return meta_features, y_meta
    
    def run_complete_evaluation_pipeline(self, 
                                       data: pd.DataFrame,
                                       save_results: bool = True) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµæ°´çº¿
        Run complete evaluation pipeline
        
        å‚æ•° Args:
            data (pd.DataFrame): åŸå§‹æ—¶é—´åºåˆ—æ•°æ® Original time series data
            save_results (bool): æ˜¯å¦ä¿å­˜ç»“æœ Whether to save results
            
        è¿”å› Returns:
            dict: å®Œæ•´è¯„ä¼°ç»“æœ Complete evaluation results
        """
        print("ğŸš€ å¼€å§‹ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿")
        print("ğŸš€ STARTING ABCM MODEL EVALUATION PIPELINE")
        print("=" * 80)
        
        # æ­¥éª¤1-2: ç‰¹å¾æå–å’Œèšç±» Steps 1-2: Feature extraction and clustering
        features, cluster_labels, grouped_data = self.extract_features_and_cluster(data)
        
        # æ­¥éª¤3: ç»¼åˆæ¨¡å‹è¯„ä¼° Step 3: Comprehensive model evaluation
        errors_by_epoch, final_error_matrix = self.comprehensive_model_evaluation(
            data, grouped_data, save_results
        )
        
        # æ­¥éª¤4: å‡†å¤‡å…ƒå­¦ä¹ æ•°æ® Step 4: Prepare meta-learning data
        X_meta, y_meta = self.prepare_meta_learning_data(
            features, cluster_labels, final_error_matrix
        )
        
        # æ•´ç†ç»“æœ Organize results
        results = {
            'features': features,
            'cluster_labels': cluster_labels,
            'grouped_data': grouped_data,
            'errors_by_epoch': errors_by_epoch,
            'final_error_matrix': final_error_matrix,
            'meta_learning_features': X_meta,
            'meta_learning_targets': y_meta,
            'n_clusters': len(np.unique(cluster_labels)),
            'n_features': features.shape[1],
            'n_models': len(self.model_evaluator.model_list)
        }
        
        print("\n" + "=" * 80)
        print("âœ… ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿å®Œæˆ")
        print("âœ… ABCM MODEL EVALUATION PIPELINE COMPLETED")
        print("=" * 80)
        print(f"èšç±»æ•°é‡ Number of clusters: {results['n_clusters']}")
        print(f"ç‰¹å¾æ•°é‡ Number of features: {results['n_features']}")
        print(f"å€™é€‰æ¨¡å‹æ•°é‡ Number of candidate models: {results['n_models']}")
        print(f"å…ƒå­¦ä¹ æ•°æ®å½¢çŠ¶ Meta-learning data shape: {X_meta.shape}")
        
        if save_results:
            # ä¿å­˜å…ƒå­¦ä¹ æ•°æ® Save meta-learning data
            X_meta.to_excel("meta_learning_features.xlsx")
            pd.DataFrame(y_meta, columns=['best_model_encoded']).to_excel("meta_learning_targets.xlsx")
            print("ğŸ“ å…ƒå­¦ä¹ æ•°æ®å·²ä¿å­˜ Meta-learning data saved")
        
        return results


def main():
    """
    ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿ä½¿ç”¨ç¤ºä¾‹
    Example usage of ABCM Model Evaluation Pipeline
    """
    print("ABCMæ¨¡å‹è¯„ä¼°æµæ°´çº¿ç¤ºä¾‹")
    print("ABCM Model Evaluation Pipeline Example")
    print("=" * 60)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿé—´æ­‡æ€§éœ€æ±‚æ•°æ® Generate simulated intermittent demand data
    np.random.seed(42)
    n_series = 100  # 100ä¸ªå¤‡ä»¶ 100 spare parts
    n_periods = 84  # 84ä¸ªæ—¶æœŸ 84 periods
    
    print("ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®... Generating simulation data...")
    
    simulation_data = pd.DataFrame()
    for i in range(n_series):
        # ç”Ÿæˆé—´æ­‡æ€§æ—¶é—´åºåˆ— Generate intermittent time series
        # ä½¿ç”¨ä¸åŒçš„å‚æ•°åˆ›å»ºä¸åŒçš„éœ€æ±‚æ¨¡å¼ Use different parameters to create different demand patterns
        if i < n_series // 3:
            # ä½é¢‘ç‡é«˜éœ€æ±‚ Low frequency high demand
            demand = np.random.poisson(2.0, n_periods)
            demand = np.where(np.random.random(n_periods) > 0.8, demand, 0)
        elif i < 2 * n_series // 3:
            # ä¸­é¢‘ç‡ä¸­éœ€æ±‚ Medium frequency medium demand
            demand = np.random.poisson(1.0, n_periods)
            demand = np.where(np.random.random(n_periods) > 0.6, demand, 0)
        else:
            # é«˜é¢‘ç‡ä½éœ€æ±‚ High frequency low demand
            demand = np.random.poisson(0.5, n_periods)
            demand = np.where(np.random.random(n_periods) > 0.4, demand, 0)
        
        simulation_data[f'spare_part_{i:03d}'] = demand
    
    print(f"âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ: {simulation_data.shape}")
    print(f"âœ… Simulation data generated: {simulation_data.shape}")
    
    # åˆå§‹åŒ–è¯„ä¼°æµæ°´çº¿ Initialize evaluation pipeline
    pipeline = ABCMModelEvaluationPipeline(
        encoding_dim=12,
        evaluation_epochs=3,  # å‡å°‘è½®æ•°ç”¨äºæ¼”ç¤º Reduce epochs for demo
        prediction_length=6
    )
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿ Run complete pipeline
    try:
        results = pipeline.run_complete_evaluation_pipeline(
            data=simulation_data,
            save_results=True
        )
        
        print("\nğŸ‰ æµæ°´çº¿è¿è¡ŒæˆåŠŸï¼ Pipeline execution successful!")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„å…ƒå­¦ä¹ æ•°æ®è®­ç»ƒAgent 3çš„å…ƒå­¦ä¹ å™¨")
        print("Now you can use the generated meta-learning data to train Agent 3's meta-learner")
        
        # æ˜¾ç¤ºä¸€äº›å…³é”®ç»Ÿè®¡ä¿¡æ¯ Display some key statistics
        print("\nğŸ“Š å…³é”®ç»Ÿè®¡ä¿¡æ¯ Key Statistics:")
        print(f"  æœ€ç»ˆé”™è¯¯çŸ©é˜µå½¢çŠ¶ Final error matrix shape: {results['final_error_matrix'].shape}")
        print(f"  å…ƒå­¦ä¹ ç‰¹å¾åˆ—æ•° Meta-learning feature columns: {results['meta_learning_features'].shape[1]}")
        print(f"  å…ƒå­¦ä¹ æ ·æœ¬æ•° Meta-learning samples: {len(results['meta_learning_targets'])}")
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå‡ºé”™ Pipeline execution error: {e}")
        print("è¯·æ£€æŸ¥ä¾èµ–åº“å®‰è£…æƒ…å†µ")
        print("Please check dependency installations")


if __name__ == "__main__":
    main() 