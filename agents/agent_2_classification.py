"""
åˆ†ç±»ä»£ç† (Agent 2) - Classification Agent

è¯¥ä»£ç†è´Ÿè´£æ ¹æ®éœ€æ±‚æ¨¡å¼å¯¹å¤‡ä»¶è¿›è¡Œåˆ†ç±»ï¼Œä½¿ç”¨PCAé™ç»´ã€ä½™å¼¦ç›¸ä¼¼æ€§å’ŒK-meansèšç±»ç­‰æ–¹æ³•
This agent is responsible for classifying spare parts based on demand patterns using
PCA dimensionality reduction, cosine similarity, and K-means clustering

ä¸»è¦åŠŸèƒ½ Main Functions:
- PCAé™ç»´ PCA dimensionality reduction
- ä½™å¼¦ç›¸ä¼¼æ€§ç‰¹å¾è®¡ç®— Cosine similarity feature computation
- K-meansèšç±» K-means clustering
- èšç±»è´¨é‡è¯„ä¼° Clustering quality evaluation
- å‘ä»£ç†1æä¾›åé¦ˆ Provide feedback to Agent 1

ä½œè€… Author: ABCM Team
åˆ›å»ºæ—¶é—´ Created: 2024
"""

import warnings
import numpy as np
import pandas as pd
from collections import Counter
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from kscorer.kscorer import KScorer

warnings.filterwarnings("ignore")


class ClassificationAgent:
    """
    ä»£ç†2ï¼šåˆ†ç±»ä»£ç†
    Agent 2: Classification Agent
    
    è¯¥ä»£ç†è´Ÿè´£åˆ©ç”¨ä»£ç†1æå–çš„ç‰¹å¾å¯¹å¤‡ä»¶è¿›è¡Œåˆ†ç±»ï¼Œé‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
    This agent categorizes spare parts using features from Agent 1, employing:
    
    1. PCAé™ç»´ - å‡å°‘ç‰¹å¾ç»´åº¦çš„åŒæ—¶ä¿æŒä¿¡æ¯
       PCA dimensionality reduction - Reduce feature dimensions while preserving information
       
    2. ä½™å¼¦ç›¸ä¼¼æ€§è®¾è®¡ - è®¡ç®—æ ·æœ¬é—´çš„ç›¸ä¼¼æ€§ç‰¹å¾
       Cosine similarity design - Compute similarity features between samples
       
    3. K-meansèšç±» - æ ¹æ®éœ€æ±‚æ¨¡å¼å°†å¤‡ä»¶åˆ†ç»„
       K-means clustering - Group spare parts based on demand patterns
       
    4. å¤šç§èšç±»è¯„ä¼°æŒ‡æ ‡ - è¯„ä¼°èšç±»è´¨é‡
       Multiple clustering evaluation metrics - Assess clustering quality
    """
    
    def __init__(self, accuracy_threshold=0.9, max_clusters=20):
        """
        åˆå§‹åŒ–åˆ†ç±»ä»£ç†
        Initialize the Classification Agent
        
        å‚æ•° Args:
            accuracy_threshold (float): èšç±»çš„æœ€å°å‡†ç¡®åº¦é˜ˆå€¼ Minimum accuracy threshold for clustering
            max_clusters (int): è€ƒè™‘çš„æœ€å¤§èšç±»æ•° Maximum number of clusters to consider
        """
        self.accuracy_threshold = accuracy_threshold
        self.max_clusters = max_clusters
        self.pca = None
        self.kmeans = None
        self.ks_scorer = None
        self.optimal_k = None
        self.cluster_labels = None
        self.cluster_metrics = {}
        
    def apply_pca(self, features, n_components=None, variance_threshold=0.95):
        """
        åº”ç”¨PCAè¿›è¡Œé™ç»´
        Apply PCA for dimensionality reduction
        
        PCAèƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®æ–¹å·®çš„åŒæ—¶å‡å°‘ç‰¹å¾ç»´åº¦ï¼Œå»é™¤å†—ä½™ä¿¡æ¯
        PCA can reduce feature dimensions while preserving data variance, removing redundant information
        
        å‚æ•° Args:
            features (pd.DataFrame): è¾“å…¥ç‰¹å¾ Input features
            n_components (int): ä¸»æˆåˆ†æ•°é‡(å¦‚æœä¸ºNoneï¼Œä½¿ç”¨æ–¹å·®é˜ˆå€¼) Number of components (if None, use variance threshold)
            variance_threshold (float): ç´¯ç§¯æ–¹å·®é˜ˆå€¼ Cumulative variance threshold
            
        è¿”å› Returns:
            pd.DataFrame: PCAå˜æ¢åçš„ç‰¹å¾ PCA-transformed features
        """
        if n_components is None:
            # æ ¹æ®æ–¹å·®é˜ˆå€¼ç¡®å®šä¸»æˆåˆ†æ•°é‡ Determine components based on variance threshold
            pca_temp = PCA()
            pca_temp.fit(features)
            cumvar = np.cumsum(pca_temp.explained_variance_ratio_)
            n_components = np.argmax(cumvar >= variance_threshold) + 1
        
        self.pca = PCA(n_components=n_components)
        pca_features = self.pca.fit_transform(features)
        
        print(f"PCAé™ç»´ï¼šä»{features.shape[1]}ç»´é™è‡³{n_components}ç»´")
        print(f"PCA: Reduced from {features.shape[1]} to {n_components} components")
        print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {sum(self.pca.explained_variance_ratio_):.3f}")
        print(f"Explained variance ratio: {sum(self.pca.explained_variance_ratio_):.3f}")
        
        pca_df = pd.DataFrame(
            pca_features, 
            columns=[f'PC{i+1}' for i in range(n_components)]
        )
        
        return pca_df
    
    def compute_cosine_similarity_features(self, features):
        """
        è®¡ç®—åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„ç‰¹å¾
        Compute cosine similarity-based features
        
        ä½™å¼¦ç›¸ä¼¼æ€§èƒ½å¤Ÿè¡¡é‡å‘é‡é—´çš„è§’åº¦ç›¸ä¼¼æ€§ï¼Œä¸å—å‘é‡é•¿åº¦å½±å“
        Cosine similarity measures angular similarity between vectors, unaffected by vector length
        
        å‚æ•° Args:
            features (pd.DataFrame): è¾“å…¥ç‰¹å¾ Input features
            
        è¿”å› Returns:
            pd.DataFrame: å¢å¼ºäº†ç›¸ä¼¼æ€§æŒ‡æ ‡çš„ç‰¹å¾ Enhanced features with similarity metrics
        """
        # è®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼æ€§ Compute pairwise cosine similarities
        similarity_matrix = cosine_similarity(features)
        
        # æå–åŸºäºç›¸ä¼¼æ€§çš„ç‰¹å¾ Extract similarity-based features
        similarity_features = pd.DataFrame({
            'mean_similarity': np.mean(similarity_matrix, axis=1),    # å¹³å‡ç›¸ä¼¼åº¦ Average similarity
            'max_similarity': np.max(similarity_matrix, axis=1),      # æœ€å¤§ç›¸ä¼¼åº¦ Maximum similarity
            'min_similarity': np.min(similarity_matrix, axis=1),      # æœ€å°ç›¸ä¼¼åº¦ Minimum similarity
            'std_similarity': np.std(similarity_matrix, axis=1)       # ç›¸ä¼¼åº¦æ ‡å‡†å·® Similarity standard deviation
        })
        
        # ä¸åŸå§‹ç‰¹å¾åˆå¹¶ Combine with original features
        enhanced_features = pd.concat([features.reset_index(drop=True), 
                                     similarity_features], axis=1)
        
        print(f"æ·»åŠ äº†4ä¸ªä½™å¼¦ç›¸ä¼¼æ€§ç‰¹å¾ Added 4 cosine similarity features")
        
        return enhanced_features
    
    def find_optimal_clusters(self, features, method='kscorer'):
        """
        ä½¿ç”¨æŒ‡å®šæ–¹æ³•å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
        Find optimal number of clusters using specified method
        
        å‚æ•° Args:
            features (pd.DataFrame): è¾“å…¥ç‰¹å¾ Input features
            method (str): å¯»æ‰¾æœ€ä¼˜èšç±»æ•°çš„æ–¹æ³• Method for finding optimal clusters
                         'kscorer': KScoreræ–¹æ³• KScorer method
                         'elbow': è‚˜éƒ¨æ³•åˆ™ Elbow method
                         'silhouette': è½®å»“ç³»æ•°æ³• Silhouette method
            
        è¿”å› Returns:
            int: æœ€ä¼˜èšç±»æ•° Optimal number of clusters
        """
        if method == 'kscorer':
            # KScoreræ˜¯ä¸“é—¨ä¸ºèšç±»ä¼˜åŒ–è®¾è®¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå‚è€ƒABCM_RAF.pyå®ç°
            # KScorer is an evaluation method specifically designed for clustering optimization, based on ABCM_RAF.py
            self.ks_scorer = KScorer()
            labels, centroids, _ = self.ks_scorer.fit_predict(features, retall=True)
            self.optimal_k = self.ks_scorer.optimal_
            
            # è·å–è¯¦ç»†çš„KScoreråˆ†æç»“æœï¼Œå‚è€ƒABCM_RAF.py
            # Get detailed KScorer analysis results, based on ABCM_RAF.py
            self.kscorer_ranked = self.ks_scorer.ranked_
            self.kscorer_peak_scores = self.ks_scorer.peak_scores_
            
            # ä¿å­˜KScorerç»“æœç”¨äºåˆ†æ Save KScorer results for analysis
            self.kscorer_results = {
                'optimal_k': self.optimal_k,
                'ranked_scores': self.kscorer_ranked,
                'peak_scores': self.kscorer_peak_scores,
                'labels': labels,
                'centroids': centroids
            }
            
            print(f"KScoreræœ€ä¼˜èšç±»æ•°: {self.optimal_k}")
            print(f"KScorer optimal clusters: {self.optimal_k}")
            print(f"KScorerå³°å€¼åˆ†æ•°: {self.kscorer_peak_scores}")
            print(f"KScorer peak scores: {self.kscorer_peak_scores}")
            
            # æ˜¾ç¤ºèšç±»åˆ†å¸ƒï¼Œå‚è€ƒABCM_RAF.py
            # Display cluster distribution, based on ABCM_RAF.py  
            from collections import Counter
            label_count = Counter(labels)
            print("èšç±»åˆ†å¸ƒ Cluster distribution:")
            for label, count in label_count.items():
                print(f"  Cluster {label}: {count} samples")
            
            return self.optimal_k
            
        elif method == 'silhouette':
            # è½®å»“ç³»æ•°æ³•ï¼šè¡¡é‡èšç±»å†…ç´§å¯†åº¦å’Œèšç±»é—´åˆ†ç¦»åº¦
            # Silhouette method: measures intra-cluster tightness and inter-cluster separation
            scores = []
            K_range = range(2, min(self.max_clusters, len(features)//2))
            
            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(features)
                score = silhouette_score(features, labels)
                scores.append(score)
            
            self.optimal_k = K_range[np.argmax(scores)]
            print(f"è½®å»“ç³»æ•°æ³•æœ€ä¼˜èšç±»æ•°: {self.optimal_k}")
            print(f"Silhouette optimal clusters: {self.optimal_k}")
            return self.optimal_k
            
        elif method == 'elbow':
            # è‚˜éƒ¨æ³•åˆ™ï¼šå¯»æ‰¾æƒ¯æ€§ä¸‹é™æ›²çº¿çš„æ‹ç‚¹
            # Elbow method: find the elbow point in the inertia decline curve
            inertias = []
            K_range = range(1, min(self.max_clusters, len(features)//2))
            
            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(features)
                inertias.append(kmeans.inertia_)
            
            # å¯»æ‰¾è‚˜éƒ¨ç‚¹(ç®€åŒ–æ–¹æ³•) Find elbow point (simplified method)
            diffs = np.diff(inertias)
            diffs2 = np.diff(diffs)
            self.optimal_k = K_range[np.argmax(diffs2) + 2]
            
            print(f"è‚˜éƒ¨æ³•åˆ™æœ€ä¼˜èšç±»æ•°: {self.optimal_k}")
            print(f"Elbow method optimal clusters: {self.optimal_k}")
            return self.optimal_k
    
    def perform_clustering(self, features, n_clusters=None, use_kscorer_labels=True):
        """
        æ‰§è¡Œèšç±»ï¼Œä¼˜å…ˆä½¿ç”¨KScorerçš„æ ‡ç­¾ç»“æœ
        Perform clustering, preferentially using KScorer label results
        
        å‚è€ƒABCM_RAF.pyçš„åšæ³•ï¼Œå¦‚æœä½¿ç”¨KScoreræ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨å…¶æ ‡ç­¾ç»“æœè€Œä¸æ˜¯é‡æ–°è¿è¡ŒK-means
        Based on ABCM_RAF.py approach, if using KScorer method, directly use its label results instead of re-running K-means
        
        å‚æ•° Args:
            features (pd.DataFrame): è¾“å…¥ç‰¹å¾ Input features
            n_clusters (int): èšç±»æ•°(å¦‚æœä¸ºNoneï¼Œåˆ™å¯»æ‰¾æœ€ä¼˜æ•°é‡) Number of clusters (if None, find optimal)
            use_kscorer_labels (bool): æ˜¯å¦ä½¿ç”¨KScorerçš„æ ‡ç­¾ç»“æœ Whether to use KScorer label results
            
        è¿”å› Returns:
            np.array: èšç±»æ ‡ç­¾ Cluster labels
        """
        if n_clusters is None:
            n_clusters = self.find_optimal_clusters(features)
        
        # å¦‚æœå·²ç»è¿è¡Œäº†KScorerå¹¶ä¸”è¦æ±‚ä½¿ç”¨å…¶æ ‡ç­¾ç»“æœ
        # If KScorer has been run and we want to use its label results
        if use_kscorer_labels and hasattr(self, 'kscorer_results') and self.kscorer_results is not None:
            print("ä½¿ç”¨KScorerçš„èšç±»æ ‡ç­¾ç»“æœ Using KScorer clustering label results")
            self.cluster_labels = self.kscorer_results['labels']
            
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„KMeanså¯¹è±¡ä»¥ä¿æŒå…¼å®¹æ€§
            # Create a dummy KMeans object for compatibility
            self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            self.kmeans.cluster_centers_ = self.kscorer_results['centroids']
            self.kmeans.labels_ = self.cluster_labels
            self.kmeans.inertia_ = np.sum([np.min([np.sum((features.iloc[i] - centroid)**2) 
                                                  for centroid in self.kscorer_results['centroids']]) 
                                          for i in range(len(features))])
        else:
            # ä½¿ç”¨ä¼ ç»ŸK-meansèšç±» Use traditional K-means clustering
            print("ä½¿ç”¨K-meansèšç±» Using K-means clustering")
            self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            self.cluster_labels = self.kmeans.fit_predict(features)
        
        # è®¡ç®—èšç±»æŒ‡æ ‡ Compute clustering metrics
        self.cluster_metrics = {
            'silhouette_score': silhouette_score(features, self.cluster_labels),
            'calinski_harabasz_score': calinski_harabasz_score(features, self.cluster_labels),
            'inertia': self.kmeans.inertia_,
            'n_clusters': n_clusters
        }
        
        # å¦‚æœæœ‰KScorerç»“æœï¼Œæ·»åŠ KScorerç›¸å…³æŒ‡æ ‡
        # If KScorer results are available, add KScorer-related metrics
        if hasattr(self, 'kscorer_results') and self.kscorer_results is not None:
            self.cluster_metrics.update({
                'kscorer_optimal_k': self.kscorer_results['optimal_k'],
                'kscorer_peak_scores': self.kscorer_results['peak_scores']
            })
        
        # æ‰“å°èšç±»åˆ†å¸ƒ Print cluster distribution
        from collections import Counter
        label_counts = Counter(self.cluster_labels)
        print(f"èšç±»åˆ†å¸ƒ Cluster distribution: {dict(label_counts)}")
        print(f"è½®å»“ç³»æ•° Silhouette score: {self.cluster_metrics['silhouette_score']:.3f}")
        
        return self.cluster_labels
    
    def evaluate_clustering_quality(self):
        """
        è¯„ä¼°èšç±»è´¨é‡å¹¶ç¡®å®šæ˜¯å¦æ»¡è¶³å‡†ç¡®åº¦é˜ˆå€¼
        Evaluate clustering quality and determine if accuracy threshold is met
        
        æ”¹è¿›ç‰ˆæœ¬ï¼Œå‚è€ƒABCM_RAF.pyçš„è¯„ä¼°æ–¹å¼ï¼Œç»“åˆKScorerå’Œä¼ ç»ŸæŒ‡æ ‡
        Improved version based on ABCM_RAF.py evaluation approach, combining KScorer and traditional metrics
        
        è¿”å› Returns:
            dict: åŒ…å«å‡†ç¡®åº¦å’Œåé¦ˆçš„è¯„ä¼°ç»“æœ Evaluation results including accuracy and feedback
        """
        if self.cluster_labels is None:
            return {'accuracy': 0, 'meets_threshold': False}
        
        # ä½¿ç”¨è½®å»“ç³»æ•°ä½œä¸ºåŸºç¡€èšç±»è´¨é‡æŒ‡æ ‡
        # Use silhouette score as base clustering quality metric
        silhouette = self.cluster_metrics['silhouette_score']
        
        # å¦‚æœæœ‰KScorerç»“æœï¼Œç»“åˆKScorerçš„å³°å€¼åˆ†æ•°è¿›è¡Œè¯„ä¼°
        # If KScorer results are available, combine with KScorer peak scores for evaluation
        if hasattr(self, 'kscorer_results') and self.kscorer_results is not None:
            # KScorerçš„å³°å€¼åˆ†æ•°åæ˜ äº†èšç±»è´¨é‡
            # KScorer peak scores reflect clustering quality
            peak_scores = self.kscorer_results['peak_scores']
            if len(peak_scores) > 0:
                max_peak_score = max(peak_scores.values()) if isinstance(peak_scores, dict) else max(peak_scores)
                # ç»“åˆè½®å»“ç³»æ•°å’ŒKScorerå³°å€¼åˆ†æ•°
                # Combine silhouette score and KScorer peak score
                kscorer_weight = 0.6  # KScoreræƒé‡ KScorer weight
                silhouette_weight = 0.4  # è½®å»“ç³»æ•°æƒé‡ Silhouette weight
                
                # æ ‡å‡†åŒ–KScorerå³°å€¼åˆ†æ•°åˆ°[0,1]èŒƒå›´
                # Normalize KScorer peak score to [0,1] range
                normalized_kscorer = min(max_peak_score / 1.0, 1.0)  # å‡è®¾æœ€å¤§å€¼ä¸º1.0
                normalized_silhouette = (silhouette + 1) / 2  # è½®å»“ç³»æ•°ä»[-1,1]æ ‡å‡†åŒ–åˆ°[0,1]
                
                combined_accuracy = (kscorer_weight * normalized_kscorer + 
                                   silhouette_weight * normalized_silhouette)
            else:
                # å¦‚æœæ²¡æœ‰å³°å€¼åˆ†æ•°ï¼Œä»…ä½¿ç”¨è½®å»“ç³»æ•°
                # If no peak scores, use only silhouette score
                combined_accuracy = (silhouette + 1) / 2
        else:
            # å°†è½®å»“ç³»æ•°æ ‡å‡†åŒ–åˆ°[0,1]èŒƒå›´(è½®å»“ç³»æ•°èŒƒå›´æ˜¯[-1,1])
            # Normalize silhouette score to [0, 1] range (silhouette is in [-1, 1])
            combined_accuracy = (silhouette + 1) / 2
        
        meets_threshold = combined_accuracy >= self.accuracy_threshold
        
        evaluation = {
            'accuracy': combined_accuracy,
            'meets_threshold': meets_threshold,
            'silhouette_score': silhouette,
            'n_clusters': self.cluster_metrics['n_clusters'],
            'cluster_metrics': self.cluster_metrics
        }
        
        # å¦‚æœæœ‰KScorerç»“æœï¼Œæ·»åŠ åˆ°è¯„ä¼°ä¸­
        # If KScorer results are available, add to evaluation
        if hasattr(self, 'kscorer_results') and self.kscorer_results is not None:
            evaluation.update({
                'kscorer_optimal_k': self.kscorer_results['optimal_k'],
                'kscorer_peak_scores': self.kscorer_results['peak_scores'],
                'kscorer_ranked': self.kscorer_results['ranked_scores']
            })
        
        return evaluation
    
    def classify(self, features, use_pca=True, use_cosine_similarity=True, clustering_method='kscorer'):
        """
        ä¸»åˆ†ç±»æ–¹æ³•ï¼Œæ•´åˆæ‰€æœ‰æŠ€æœ¯
        Main classification method that integrates all techniques
        
        è¿™æ˜¯åˆ†ç±»ä»£ç†çš„æ ¸å¿ƒæ–¹æ³•ï¼Œæ•´åˆäº†æ‰€æœ‰åˆ†ç±»æŠ€æœ¯ï¼Œé»˜è®¤ä½¿ç”¨KScoreræ–¹æ³•è¿›è¡Œèšç±»
        å‚è€ƒABCM_RAF.pyå’Œclassification-US.pyä¸­çš„å®ç°
        This is the core method of the classification agent, integrating all classification techniques, 
        using KScorer method by default, based on ABCM_RAF.py and classification-US.py implementations
        
        å‚æ•° Args:
            features (pd.DataFrame): æ¥è‡ªä»£ç†1çš„è¾“å…¥ç‰¹å¾ Input features from Agent 1
            use_pca (bool): æ˜¯å¦åº”ç”¨PCA Whether to apply PCA
            use_cosine_similarity (bool): æ˜¯å¦ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§ç‰¹å¾ Whether to use cosine similarity features
            clustering_method (str): èšç±»æ–¹æ³• Clustering method ('kscorer', 'silhouette', 'elbow')
            
        è¿”å› Returns:
            tuple: (cluster_labels, evaluation_results) èšç±»æ ‡ç­¾å’Œè¯„ä¼°ç»“æœ
        """
        print("å¼€å§‹åˆ†ç±»è¿‡ç¨‹... Starting classification process...")
        print(f"ä½¿ç”¨èšç±»æ–¹æ³• Using clustering method: {clustering_method}")
        
        # é¢„å¤„ç†ç‰¹å¾ Preprocess features
        processed_features = features.copy()
        
        # ç§»é™¤ä»»ä½•NaNå€¼ Remove any NaN values
        processed_features = processed_features.dropna()
        
        if use_cosine_similarity:
            print("è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§ç‰¹å¾... Computing cosine similarity features...")
            processed_features = self.compute_cosine_similarity_features(processed_features)
        
        if use_pca and processed_features.shape[1] > 10:
            print("åº”ç”¨PCA... Applying PCA...")
            processed_features = self.apply_pca(processed_features)
        
        # ä½¿ç”¨æŒ‡å®šçš„æ–¹æ³•å¯»æ‰¾æœ€ä¼˜èšç±»æ•° Find optimal clusters using specified method
        print(f"ä½¿ç”¨{clustering_method}æ–¹æ³•å¯»æ‰¾æœ€ä¼˜èšç±»æ•°...")
        print(f"Finding optimal clusters using {clustering_method} method...")
        optimal_k = self.find_optimal_clusters(processed_features, method=clustering_method)
        
        # æ‰§è¡Œèšç±» Perform clustering
        print("æ‰§è¡Œèšç±»... Performing clustering...")
        cluster_labels = self.perform_clustering(processed_features, n_clusters=optimal_k)
        
        # è¯„ä¼°èšç±»è´¨é‡ Evaluate clustering quality
        evaluation = self.evaluate_clustering_quality()
        
        print(f"åˆ†ç±»å®Œæˆã€‚å‡†ç¡®åº¦: {evaluation['accuracy']:.3f}")
        print(f"Classification completed. Accuracy: {evaluation['accuracy']:.3f}")
        
        # å¦‚æœä½¿ç”¨äº†KScorerï¼Œæ˜¾ç¤ºè¯¦ç»†åˆ†æ
        # If KScorer was used, show detailed analysis
        if clustering_method == 'kscorer' and hasattr(self, 'kscorer_results'):
            print(f"KScoreræœ€ä¼˜èšç±»æ•°: {optimal_k}")
            print(f"KScorer optimal clusters: {optimal_k}")
            if hasattr(self, 'kscorer_peak_scores'):
                print(f"KScorerå³°å€¼åˆ†æ•°: {self.kscorer_peak_scores}")
        
        return cluster_labels, evaluation
    
    def provide_feedback_to_agent1(self, evaluation):
        """
        æ ¹æ®èšç±»è¯„ä¼°å‘ä»£ç†1æä¾›åé¦ˆä¿¡å·
        Provide feedback signal to Agent 1 based on clustering evaluation
        
        å‚æ•° Args:
            evaluation (dict): èšç±»è¯„ä¼°ç»“æœ Clustering evaluation results
            
        è¿”å› Returns:
            dict: ç»™ä»£ç†1çš„åé¦ˆä¿¡å· Feedback signal for Agent 1
        """
        feedback = {
            'accuracy': evaluation['accuracy'],
            'meets_threshold': evaluation['meets_threshold']
        }
        
        if not evaluation['meets_threshold']:
            # æ ¹æ®èšç±»ç»“æœå»ºè®®æ”¹è¿› Suggest improvements based on clustering results
            if evaluation['silhouette_score'] < 0.3:
                feedback['suggestion'] = 'adjust_autoencoder_dim'
                feedback['message'] = "è½®å»“ç³»æ•°è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´è‡ªç¼–ç å™¨ç»´åº¦"
            elif evaluation['n_clusters'] < 3:
                feedback['suggestion'] = 'add_features'
                feedback['message'] = "èšç±»æ•°é‡è¿‡å°‘ï¼Œå»ºè®®æ·»åŠ æ›´å¤šç‰¹å¾"
            else:
                feedback['suggestion'] = 'adjust_autoencoder_dim'
                feedback['message'] = "èšç±»è´¨é‡ä¸ä½³ï¼Œå°è¯•è°ƒæ•´è‡ªç¼–ç å™¨ç»´åº¦"
                
            feedback['message'] += f" | èšç±»å‡†ç¡®åº¦ {evaluation['accuracy']:.3f} ä½äºé˜ˆå€¼ {self.accuracy_threshold}"
        else:
            feedback['suggestion'] = 'maintain_strategy'
            feedback['message'] = "èšç±»æ»¡è¶³å‡†ç¡®åº¦è¦æ±‚ Clustering meets accuracy requirements"
        
        return feedback
    
    def predict_new_data(self, new_features):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ–°æ•°æ®çš„èšç±»æ ‡ç­¾
        Predict cluster labels for new data using trained models
        
        å‚è€ƒclassification-US.pyä¸­çš„åšæ³•ï¼Œä¼˜å…ˆä½¿ç”¨KScorerè¿›è¡Œé¢„æµ‹
        Based on classification-US.py approach, preferentially use KScorer for prediction
        
        å‚æ•° Args:
            new_features (pd.DataFrame): è¦åˆ†ç±»çš„æ–°ç‰¹å¾ New features to classify
            
        è¿”å› Returns:
            np.array: é¢„æµ‹çš„èšç±»æ ‡ç­¾ Predicted cluster labels
        """
        processed_features = new_features.copy().dropna()
        
        # å¦‚æœä½¿ç”¨äº†PCAï¼Œéœ€è¦å…ˆè¿›è¡ŒPCAå˜æ¢
        # If PCA was used, apply PCA transformation first
        if self.pca is not None:
            processed_features = pd.DataFrame(
                self.pca.transform(processed_features),
                columns=[f'PC{i+1}' for i in range(self.pca.n_components_)]
            )
        
        # ä¼˜å…ˆä½¿ç”¨KScorerè¿›è¡Œé¢„æµ‹ï¼Œå‚è€ƒclassification-US.py
        # Preferentially use KScorer for prediction, based on classification-US.py
        if self.ks_scorer is not None:
            try:
                print("ä½¿ç”¨KScoreré¢„æµ‹æ–°æ•°æ®æ ‡ç­¾ Using KScorer to predict new data labels")
                predicted_labels = self.ks_scorer.predict(processed_features, retall=False)
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœåˆ†å¸ƒ
                # Display prediction result distribution
                from collections import Counter
                label_count = Counter(predicted_labels)
                print("æ–°æ•°æ®é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ New data prediction label distribution:")
                for label, count in label_count.items():
                    print(f"  Cluster {label}: {count} samples")
                
                return predicted_labels
            except Exception as e:
                print(f"KScoreré¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°K-meansæ–¹æ³•: {e}")
                print(f"KScorer prediction failed, falling back to K-means: {e}")
        
        # å¦‚æœKScorerä¸å¯ç”¨ï¼Œä½¿ç”¨K-meansè¿›è¡Œé¢„æµ‹
        # If KScorer is not available, use K-means for prediction
        if self.kmeans is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨classify()æ–¹æ³•ã€‚Model not trained. Call classify() first.")
        
        print("ä½¿ç”¨K-meansé¢„æµ‹æ–°æ•°æ®æ ‡ç­¾ Using K-means to predict new data labels")
        predicted_labels = self.kmeans.predict(processed_features)
        
        return predicted_labels
    
    def save_model(self, filename):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æ–‡ä»¶ Save trained models to file"""
        import pickle
        model_data = {
            'pca': self.pca,
            'kmeans': self.kmeans,
            'ks_scorer': self.ks_scorer,
            'optimal_k': self.optimal_k,
            'cluster_metrics': self.cluster_metrics,
            'accuracy_threshold': self.accuracy_threshold,
            # æ·»åŠ KScorerç›¸å…³ç»“æœ Add KScorer-related results
            'kscorer_results': getattr(self, 'kscorer_results', None),
            'kscorer_ranked': getattr(self, 'kscorer_ranked', None),
            'kscorer_peak_scores': getattr(self, 'kscorer_peak_scores', None)
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"åˆ†ç±»æ¨¡å‹å·²ä¿å­˜åˆ°: {filename}")
        print(f"Classification model saved to: {filename}")
        
        # å¦‚æœæœ‰KScorerç»“æœï¼Œä¹Ÿä¿å­˜è¯¦ç»†çš„åˆ†æç»“æœ
        # If KScorer results are available, also save detailed analysis results
        if hasattr(self, 'kscorer_ranked') and self.kscorer_ranked is not None:
            ranked_filename = filename.replace('.pkl', '_kscorer_ranked.xlsx')
            self.kscorer_ranked.to_excel(ranked_filename)
            print(f"KScorerè¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {ranked_filename}")
            print(f"KScorer detailed analysis results saved to: {ranked_filename}")
    
    def load_model(self, filename):
        """ä»æ–‡ä»¶åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ Load trained models from file"""
        import pickle
        with open(filename, 'rb') as f:
            model_data = pickle.load(f)
        
        self.pca = model_data.get('pca')
        self.kmeans = model_data.get('kmeans')
        self.ks_scorer = model_data.get('ks_scorer')
        self.optimal_k = model_data.get('optimal_k')
        self.cluster_metrics = model_data.get('cluster_metrics', {})
        self.accuracy_threshold = model_data.get('accuracy_threshold', 0.9)
        
        # åŠ è½½KScorerç›¸å…³ç»“æœ Load KScorer-related results
        self.kscorer_results = model_data.get('kscorer_results')
        self.kscorer_ranked = model_data.get('kscorer_ranked')
        self.kscorer_peak_scores = model_data.get('kscorer_peak_scores')
        
        print(f"åˆ†ç±»æ¨¡å‹å·²ä» {filename} åŠ è½½")
        print(f"Classification model loaded from {filename}")
        
        if self.kscorer_results is not None:
            print(f"KScoreræœ€ä¼˜èšç±»æ•°: {self.kscorer_results['optimal_k']}")
            print(f"KScorer optimal clusters: {self.kscorer_results['optimal_k']}")
    
    def show_kscorer_analysis(self, save_chart=False, chart_filename=None):
        """
        æ˜¾ç¤ºKScoreråˆ†æç»“æœï¼Œå‚è€ƒABCM_RAF.pyä¸­çš„show()æ–¹æ³•
        Display KScorer analysis results, based on show() method in ABCM_RAF.py
        
        å‚æ•° Args:
            save_chart (bool): æ˜¯å¦ä¿å­˜å›¾è¡¨ Whether to save chart
            chart_filename (str): å›¾è¡¨ä¿å­˜æ–‡ä»¶å Chart save filename
        """
        if self.ks_scorer is None:
            print("KScoreræœªè¿è¡Œï¼Œæ— æ³•æ˜¾ç¤ºåˆ†æç»“æœ")
            print("KScorer not run, cannot display analysis results")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š KScorer èšç±»åˆ†æç»“æœ KScorer Clustering Analysis Results")
        print("="*60)
        
        try:
            # æ˜¾ç¤ºKScorerå›¾è¡¨ï¼Œå‚è€ƒABCM_RAF.py
            # Display KScorer chart, based on ABCM_RAF.py
            self.ks_scorer.show()
            
            print(f"\næœ€ä¼˜èšç±»æ•° Optimal clusters: {self.optimal_k}")
            
            if hasattr(self, 'kscorer_peak_scores') and self.kscorer_peak_scores:
                print("\nå³°å€¼åˆ†æ•° Peak scores:")
                if isinstance(self.kscorer_peak_scores, dict):
                    for k, score in self.kscorer_peak_scores.items():
                        print(f"  K={k}: {score:.4f}")
                else:
                    print(f"  {self.kscorer_peak_scores}")
            
            if hasattr(self, 'kscorer_ranked') and self.kscorer_ranked is not None:
                print(f"\næ’åç»“æœå½¢çŠ¶ Ranked results shape: {self.kscorer_ranked.shape}")
                print("å‰5ä¸ªèšç±»æ•°çš„ç»¼åˆå¾—åˆ† Top 5 cluster numbers' scores:")
                print(self.kscorer_ranked.head())
            
            print("="*60)
            
        except Exception as e:
            print(f"æ˜¾ç¤ºKScoreråˆ†æç»“æœæ—¶å‡ºé”™: {e}")
            print(f"Error displaying KScorer analysis results: {e}")
    
    def export_kscorer_results(self, base_filename):
        """
        å¯¼å‡ºKScorerç»“æœåˆ°Excelæ–‡ä»¶ï¼Œå‚è€ƒABCM_RAF.pyçš„åšæ³•
        Export KScorer results to Excel files, based on ABCM_RAF.py approach
        
        å‚æ•° Args:
            base_filename (str): åŸºç¡€æ–‡ä»¶å Base filename
        """
        if not hasattr(self, 'kscorer_results') or self.kscorer_results is None:
            print("æ²¡æœ‰KScorerç»“æœå¯å¯¼å‡º")
            print("No KScorer results to export")
            return
        
        try:
            # å¯¼å‡ºæ’åç»“æœ Export ranked results
            if hasattr(self, 'kscorer_ranked') and self.kscorer_ranked is not None:
                ranked_filename = f"{base_filename}_kscorer_ranked.xlsx"
                self.kscorer_ranked.to_excel(ranked_filename)
                print(f"KScoreræ’åç»“æœå·²ä¿å­˜åˆ°: {ranked_filename}")
                print(f"KScorer ranked results saved to: {ranked_filename}")
            
            # å¯¼å‡ºèšç±»æ ‡ç­¾ Export cluster labels
            if 'labels' in self.kscorer_results:
                labels_df = pd.DataFrame(
                    self.kscorer_results['labels'], 
                    columns=['cluster_label']
                )
                labels_filename = f"{base_filename}_cluster_labels.xlsx"
                labels_df.to_excel(labels_filename)
                print(f"èšç±»æ ‡ç­¾å·²ä¿å­˜åˆ°: {labels_filename}")
                print(f"Cluster labels saved to: {labels_filename}")
            
            # å¯¼å‡ºå³°å€¼åˆ†æ•° Export peak scores
            if 'peak_scores' in self.kscorer_results:
                peak_scores_df = pd.DataFrame([self.kscorer_results['peak_scores']])
                peak_filename = f"{base_filename}_peak_scores.xlsx"
                peak_scores_df.to_excel(peak_filename)
                print(f"å³°å€¼åˆ†æ•°å·²ä¿å­˜åˆ°: {peak_filename}")
                print(f"Peak scores saved to: {peak_filename}")
                
        except Exception as e:
            print(f"å¯¼å‡ºKScorerç»“æœæ—¶å‡ºé”™: {e}")
            print(f"Error exporting KScorer results: {e}") 